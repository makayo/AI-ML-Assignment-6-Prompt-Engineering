{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36906773-0494-42be-90a6-e55a24ec3244",
   "metadata": {},
   "source": [
    "# LLM Task Optimization\n",
    "This notebook demonstrates how prompt engineering techniques improve the performance of a Large Language Model (LLM).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffc606fb-8c3c-4aaf-bf88-3172c18c9e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\marky\\anaconda3\\lib\\site-packages (4.57.1)\n",
      "Requirement already satisfied: torch in c:\\users\\marky\\anaconda3\\lib\\site-packages (2.9.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\marky\\anaconda3\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\marky\\anaconda3\\lib\\site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\marky\\anaconda3\\lib\\site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\marky\\anaconda3\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\marky\\anaconda3\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\marky\\anaconda3\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\marky\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\marky\\anaconda3\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\marky\\anaconda3\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\marky\\anaconda3\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\marky\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\marky\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\marky\\anaconda3\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\marky\\anaconda3\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\marky\\anaconda3\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\marky\\anaconda3\\lib\\site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\marky\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\marky\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\marky\\anaconda3\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\marky\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\marky\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\marky\\anaconda3\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\marky\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.11.12)\n"
     ]
    }
   ],
   "source": [
    "# Run this in a Jupyter cell with ! or in your terminal\n",
    "!pip install transformers torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54704455-2c7b-48e9-99fc-7388ec5f95e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f622d366cb94e8cb4e801913d209b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marky\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\marky\\.cache\\huggingface\\hub\\models--google--flan-t5-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "409a10f986334fb9b7ede5dae902b045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faf05c093d4f45d08eb4198da2272e6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8602a65a6448425f81466ce4be753566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f835241813da424999f828c881436394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c45963c479114406a7b46a7edca19260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88f50460afd046f29b37ea8bd55b65a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# 1. Import libraries\n",
    "from transformers import pipeline\n",
    "\n",
    "# 2. Load a pre-trained LLM (you can swap models depending on task complexity)\n",
    "# For text generation / reasoning tasks:\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "# For instruction-following tasks (better for prompt engineering):\n",
    "# Try a model like \"google/flan-t5-large\" or \"facebook/bart-large\"\n",
    "qa_model = pipeline(\"text2text-generation\", model=\"google/flan-t5-large\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f06f6fab-8a30-4287-888a-c0e04dd79f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Baseline Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bc85674-d37e-4c79-a5a4-d45dd923b2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Output: Best Buy, store, Best Buy; Samsung Galaxy S23, price, \"799 at Best Buy\"; Samsung Galaxy S23, purchaseDate, \"November 15, 2025\"\n"
     ]
    }
   ],
   "source": [
    "# Input text (unstructured review)\n",
    "input_text = \"I bought the Samsung Galaxy S23 last week for $799 at Best Buy. The purchase date was November 15, 2025.\"\n",
    "\n",
    "# Baseline prompt\n",
    "baseline_prompt = \"Extract the data.\"\n",
    "\n",
    "baseline_output = qa_model(baseline_prompt + \"\\n\\n\" + input_text, max_length=100)\n",
    "print(\"Baseline Output:\", baseline_output[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18296e80-611a-4347-8478-0bfc0bf6d772",
   "metadata": {},
   "source": [
    "# Iterative Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486e07e4-7ffb-41b4-9440-240db75fde3d",
   "metadata": {},
   "source": [
    "## Technique 1: Role Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6943447f-d8cd-41f1-aae4-91a9a30c6480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role Prompting Output: Samsung Galaxy S23, price, Best Buy, November 15, 2025\n"
     ]
    }
   ],
   "source": [
    "role_prompt = \"\"\"You are a Senior Data Analyst. \n",
    "Extract the product name, price, and purchase date from the following review.\"\"\"\n",
    "\n",
    "role_output = qa_model(role_prompt + \"\\n\\n\" + input_text, max_length=100)\n",
    "print(\"Role Prompting Output:\", role_output[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5640158-9a20-4eba-8491-567fefbc331e",
   "metadata": {},
   "source": [
    "# Output Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a6cae85-ceb8-4068-9d62-a29f1bee75b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Formatting: Name[Samsung Galaxy S23], Price[799], PurchaseDate[November 15, 2025]\n"
     ]
    }
   ],
   "source": [
    "format_prompt = \"\"\"Extract the product name, price, and purchase date from the following review. \n",
    "Provide the output strictly in JSON format with keys: Name, Price, Date.\"\"\"\n",
    "\n",
    "format_output = qa_model(format_prompt + \"\\n\\n\" + input_text, max_length=100)\n",
    "print(\"Output Formatting:\", format_output[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a333cc-fdab-4964-8b21-0ca41bb3aecd",
   "metadata": {},
   "source": [
    "# Chain-of-Thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fc45166-c187-4edb-8e04-92bc0a4b7d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain-of-Thought Output: Name[Samsung Galaxy S23], Price[799], PurchaseDate[November 15, 2025]\n"
     ]
    }
   ],
   "source": [
    "cot_prompt = \"\"\"First, list the steps you will take to extract the product name, price, and purchase date. \n",
    "Then provide the final answer in JSON format with keys: Name, Price, Date.\"\"\"\n",
    "\n",
    "cot_output = qa_model(cot_prompt + \"\\n\\n\" + input_text, max_length=150)\n",
    "print(\"Chain-of-Thought Output:\", cot_output[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a0288d-b32e-43f8-872e-8d5b1d05f994",
   "metadata": {},
   "source": [
    "# Final Optimized Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02085da6-c894-47cd-a2ab-0b6cb64df34d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Optimized Output: Samsung Galaxy S23, Price, PurchaseDate, \"November 15, 2025\"\n"
     ]
    }
   ],
   "source": [
    "final_prompt = \"\"\"You are a Senior Data Analyst. Carefully extract the product name, price, and purchase date from the following review. \n",
    "Before answering, think step by step to ensure accuracy. \n",
    "Provide the final output strictly in JSON format with keys: Name, Price, Date. \n",
    "Normalize the price as a number (no currency symbol) and format the date in ISO (YYYY-MM-DD).\"\"\"\n",
    "\n",
    "final_output = qa_model(final_prompt + \"\\n\\n\" + input_text, max_length=150)\n",
    "print(\"Final Optimized Output:\", final_output[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc52683-5ddd-4892-8bb9-5607eca4e104",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
